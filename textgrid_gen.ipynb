{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading cdict1.xls\n",
      "Reading cdict2.xls\n",
      "Reading cdict3.xls\n",
      "        字詞名               注音一式\n",
      "0         八                 ㄅㄚ\n",
      "1         扒                 ㄅㄚ\n",
      "2         叭                 ㄅㄚ\n",
      "3         巴                 ㄅㄚ\n",
      "4         吧                 ㄅㄚ\n",
      "5         芭                 ㄅㄚ\n",
      "6         疤                 ㄅㄚ\n",
      "7         羓                 ㄅㄚ\n",
      "8         粑                 ㄅㄚ\n",
      "9         笆                 ㄅㄚ\n",
      "10        豝                 ㄅㄚ\n",
      "11        鈀                 ㄅㄚ\n",
      "12        捌                 ㄅㄚ\n",
      "13        拔                ㄅㄚˊ\n",
      "14        茇                ㄅㄚˊ\n",
      "15        胈                ㄅㄚˊ\n",
      "16        軷                ㄅㄚˊ\n",
      "17        菝                ㄅㄚˊ\n",
      "18        跋                ㄅㄚˊ\n",
      "19        鈸                ㄅㄚˊ\n",
      "20        魃                ㄅㄚˊ\n",
      "21        鼥                ㄅㄚˊ\n",
      "22        把                ㄅㄚˇ\n",
      "23        鈀                ㄅㄚˇ\n",
      "24        靶                ㄅㄚˇ\n",
      "25        把                ㄅㄚˋ\n",
      "26        弝                ㄅㄚˋ\n",
      "27        爸                ㄅㄚˋ\n",
      "28        耙                ㄅㄚˋ\n",
      "29        伯                ㄅㄚˋ\n",
      "...     ...                ...\n",
      "45990     屯               ㄔㄨㄣˊ\n",
      "45991   內行星       ㄋㄟˋ　ㄒㄧㄥˊ　ㄒㄧㄥ\n",
      "45992  九大行星  ㄐㄧㄡˇ　ㄉㄚˋ　ㄒㄧㄥˊ　ㄒㄧㄥ\n",
      "45993   矮行星        ㄞˇ　ㄒㄧㄥˊ　ㄒㄧㄥ\n",
      "45995   新北市         ㄒㄧㄣ　ㄅㄟˇ　ㄕˋ\n",
      "45996    一著             ㄧ　ㄓㄨㄛˊ\n",
      "45997  北迴歸線  ㄅㄟˇ　ㄏㄨㄟˊ　ㄍㄨㄟ　ㄒㄧㄢˋ\n",
      "45998    礱磨           ㄌㄨㄥˊ　ㄇㄛˋ\n",
      "45999    差等             ㄔㄚ　ㄉㄥˇ\n",
      "46000    妻子              ㄑㄧ　˙ㄗ\n",
      "46001  三復白圭    ㄙㄢˋ　ㄈㄨˋ　ㄅㄞˊ　ㄍㄨㄟ\n",
      "46002    沒亂           ㄇㄛˋ　ㄌㄨㄢˋ\n",
      "46003   沒亂殺        ㄇㄛˋ　ㄌㄨㄢˋ　ㄕㄚ\n",
      "46004   沒亂煞        ㄇㄛˋ　ㄌㄨㄢˋ　ㄕㄚ\n",
      "46005    大都             ㄉㄚˋ　ㄉㄡ\n",
      "46006    綞子            ㄉㄨㄛˇ　˙ㄗ\n",
      "46007     枓                ㄉㄡˇ\n",
      "46008     郇               ㄏㄨㄢˊ\n",
      "46009     厘                ㄌㄧˊ\n",
      "46010    厘米            ㄌㄧˊ　ㄇㄧˇ\n",
      "46011    冽清           ㄌㄧㄝˋ　ㄑㄧㄥ\n",
      "46012    飛子              ㄈㄟ　ㄗˇ\n",
      "46013   不可當         ㄅㄨˋ　ㄎㄜˇ　ㄉㄤ\n",
      "46014   野老兒          ㄧㄝˇ　ㄌㄠˇ ㄦ\n",
      "46015     歂               ㄔㄨㄢˇ\n",
      "46016    強人           ㄑㄧㄤˇ　ㄖㄣˊ\n",
      "46017     箬               ㄖㄨㄛˋ\n",
      "46018  既往不究   ㄐㄧˋ　ㄨㄤˇ　ㄅㄨˋ　ㄐㄧㄡˋ\n",
      "46020    老狠            ㄌㄠˇ　ㄏㄣˇ\n",
      "46021    野食             ㄧㄝˇ　ㄕˊ\n",
      "\n",
      "[163818 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_dict():\n",
    "    dicts=[\"cdict1.xls\", \"cdict2.xls\", \"cdict3.xls\"]\n",
    "    cdict = pd.DataFrame()\n",
    "    for d in dicts:\n",
    "        print(\"Reading\", d)\n",
    "        cdp = pd.read_excel(d)\n",
    "        cdict = cdict.append(cdp)\n",
    "    cdict = cdict.drop(columns=[\"字詞屬性\", \"字詞號\", \"部首字\", \"部首外筆劃數\", \"總筆劃數\",\"漢語拼音\", \"相似詞\", \"相反詞\", \"釋義\", \"編按\", \"多音參見訊息\", \"異體字\"])\n",
    "    cdict = cdict[~cdict['字詞名'].str.contains('gif')]\n",
    "    cdict = cdict[~cdict['字詞名'].str.contains('png')]\n",
    "    cdict = cdict.replace(to_replace ='ㄦ', value = ' ㄦ', regex = True)\n",
    "    cdict = cdict.replace(to_replace =',', value = '', regex = True)\n",
    "    cdict = cdict.replace(to_replace =\"[\\（\\(\\[].*?[\\）\\)\\]]\", value = \"\", regex = True)\n",
    "    cdict = cdict.dropna()\n",
    "    #print(cdict)\n",
    "    return cdict\n",
    "\n",
    "cdx = read_dict()\n",
    "\n",
    "def four_sounds(str):\n",
    "    if 'ˋ' in str:\n",
    "        return 4 # can return 51\n",
    "    elif 'ˊ' in str:\n",
    "        return 2 # can return 35\n",
    "    elif 'ˇ' in str:\n",
    "        return 3 # can return 21\n",
    "    elif '˙' in str:\n",
    "        return 0 # can return 0\n",
    "    else:\n",
    "        return 1 # can return 55\n",
    "    \n",
    "def contain_chinese(check_str):\n",
    "    \"\"\"\n",
    "    判断字符串中是否包含中文\n",
    "    :param check_str: {str} 需要检测的字符串\n",
    "    :return: {bool} 包含返回True， 不包含返回False\n",
    "    \"\"\"\n",
    "    for ch in check_str:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引用套件並縮寫為 pd  \n",
    "import pandas as pd\n",
    "#讀入 EHowNet\n",
    "from ehownet_python3 import *\n",
    "import re\n",
    "\n",
    "def read_ehownet():\n",
    "    try:\n",
    "        EHowTree=EHowNetTree(\"db/ehownet_ontology.sqlite\")\n",
    "        #tree=EHowNetTree(\"db/ehownet_ontology_sim.sqlite\")\n",
    "        #print(dict)\n",
    "        \n",
    "        return EHowTree\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        exit(-1)\n",
    "\n",
    "def build_sampa_table():\n",
    "    try:\n",
    "        sampadict = pd.read_excel(\"SAMPA_revised.xlsx\")\n",
    "        #ssampa = sampadict.sort_values(by=SAMPA.str.len())\n",
    "        s=sampadict.SAMPA.str.len().sort_values(ascending=False).index\n",
    "        smp = sampadict.reindex(s)\n",
    "        smp = smp.reset_index(drop=True)\n",
    "        #print(smp)\n",
    "    \n",
    "        ptnstr = smp['SAMPA'].to_string(index=False).replace(\"\\n\",\"|\").replace(\" \",\"\")\n",
    "        #print(ptnstr)\n",
    "        #newidx = sampadict.SAMPA.str.len().sort_values(ascending=False)\n",
    "        ptn = re.compile(ptnstr)\n",
    "    \n",
    "        return (ptn, smp)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "from opencc import OpenCC\n",
    "\n",
    "def file_translation(fn):\n",
    "    try:\n",
    "        infile = fn\n",
    "        rawdata = open(infile, \"rb\").read()\n",
    "        result = chardet.detect(rawdata)\n",
    "        charenc = result['encoding']\n",
    "        #print(\"The file encoding:\", charenc)\n",
    "        if charenc != 'UTF-16':\n",
    "            print(\"Suspecious File format: \", charenc)\n",
    "            exit(-1)\n",
    "            \n",
    "        # Here we assume that the original contains simplifed chinese, the conversion result is UTF-8 tranditional chinese\n",
    "        cc = OpenCC('s2t')\n",
    "        source = open(infile, 'r', encoding = charenc)\n",
    "        result = open(infile+\".utf8\", 'w', encoding = 'utf-8')\n",
    "        #source就放純文字檔，轉完就放進去result\n",
    "        count = 0\n",
    "        while True:\n",
    "            line = source.readline()\n",
    "            line = cc.convert(line)\n",
    "            if not line:  #readline會一直讀下去，這邊做的break\n",
    "                break\n",
    "            #print(line)\n",
    "            count = count +1\n",
    "            result.write(line) \n",
    "            #print('===已處理'+str(count)+'行===')\n",
    "        source.close()        \n",
    "        result.close()\n",
    "        print(\"Chinese conversion complete\")\n",
    "    except Exception as e:        \n",
    "        print(e)\n",
    "        exit(-1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import sys\n",
    "import tgt\n",
    "\n",
    "EXTENSION = 'TextGrid'\n",
    "\n",
    "def print_tiernames(filenames):\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            tg = tgt.io.read_textgrid(filename)\n",
    "            print(filename)\n",
    "            for tiername in tg.get_tier_names():\n",
    "                print('\\t' + tiername)\n",
    "        except err:\n",
    "            print(filename + ' caused a problem.')\n",
    "            sys.stderr.write('ERROR: %s\\n' % str(err))\n",
    "\n",
    "def read_txtgrid(filename):\n",
    "    tg = tgt.io.read_textgrid(filename)\n",
    "    return tg\n",
    "\n",
    "def add_text_tier(tg, NNM=\"\"):\n",
    "    new_tier = tgt.core.PointTier(name=NNM)\n",
    "    tg.add_tier(new_tier)\n",
    "    return new_tier\n",
    "\n",
    "def add_interval_tier(tg, NNM=\"\", STT=0, ETT=0):\n",
    "    new_tier = tgt.core.IntervalTier(name=NNM, start_time = STT, end_time = ETT)\n",
    "    tg.add_tier(new_tier)\n",
    "    return new_tier\n",
    "\n",
    "def print_tier(tg):\n",
    "    for tiername in tg.get_tier_names():\n",
    "        print('\\t' + tiername)   \n",
    "\n",
    "def write_txtgrid(tg, NNM):\n",
    "    tgt.io.write_to_file(tg, filename=NNM )\n",
    "    \n",
    "def remove_old_tier(tg, name):\n",
    "    if (tg.has_tier(name)):\n",
    "        tg.delete_tier(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contain_chinese(check_str):\n",
    "    \"\"\"\n",
    "    判断字符串中是否包含中文\n",
    "    :param check_str: {str} 需要检测的字符串\n",
    "    :return: {bool} 包含返回True， 不包含返回False\n",
    "    \"\"\"\n",
    "    for ch in check_str:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def remove_noise(tr):\n",
    "    for o in tr._objects:\n",
    "        if o.duration() < 0.01:\n",
    "            tr.delete_annotation_by_start_time(o.start_time)\n",
    "\n",
    "def filling_gaps(tr):\n",
    "    tr.start_time = 0\n",
    "    o = tr._objects[0]\n",
    "    if o.start_time != 0:\n",
    "        o.start_time = 0\n",
    "    o = tr._objects[-1]\n",
    "    if o.end_time != tr.end_time:\n",
    "        o.end_time = tr.end_time\n",
    "    print(tr)\n",
    "    idx = len(tr._objects)\n",
    "    for i in range(1,idx):\n",
    "        if tr._objects[i].start_time != tr._objects[i-1].end_time:\n",
    "            tr._objects[i].start_time = tr._objects[i-1].end_time\n",
    "\n",
    "def align_tiers(wtr, ptr):\n",
    "    for w in wtr._objects:\n",
    "        wst = w.start_time\n",
    "        wet = w.end_time\n",
    "        pann = ptr.get_annotations_between_timepoints(w.start_time, w.end_time, False, False)\n",
    "        #print(wst, wet, pann[0].start_time, pann[-1].end_time)\n",
    "        \n",
    "        #print(\"Wst:\", math.isclose(wst, pann[0].start_time), wst, pann[0].start_time)\n",
    "        if not math.isclose(wst, pann[0].start_time):\n",
    "            pann2 = ptr.get_annotations_between_timepoints(w.start_time, w.end_time, True, False)\n",
    "            pst1 = pann[0].start_time\n",
    "            pst2 = pann2[0].start_time\n",
    "            #print(\"S:\", w.start_time, pann[0].start_time, pann2[0].start_time)\n",
    "            if abs(wst - pst1) < abs(wst - pst2):\n",
    "                pann[0].start_time = wst\n",
    "            else:\n",
    "                pann2[0].start_time = wst\n",
    "        \n",
    "        #print(\"Wet:\", math.isclose(wet, pann[-1].end_time), wet, pann[-1].end_time)\n",
    "        if  not math.isclose(wet, pann[-1].end_time):\n",
    "            pann2 = ptr.get_annotations_between_timepoints(w.start_time, w.end_time, False, True)\n",
    "            pet1 = pann[-1].end_time\n",
    "            pet2 = pann2[-1].end_time\n",
    "            #print(\"E: \", wet, pann[-1].end_time, pann2[-1].end_time)\n",
    "            if abs(wet - pet1) < abs(wet - pet2):\n",
    "                pann[-1].end_time = wet\n",
    "            else:\n",
    "                pann2[-1].end_time = wet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampa_to_cgvn(sa1, smp, ptn):\n",
    "    idx = 0\n",
    "    ipastr = \"\"\n",
    "    while idx < len(sa1):\n",
    "        ma = re.match(ptn, sa1[idx:])\n",
    "        ipatoken = smp[smp['SAMPA']==ma.group()].Syllable.to_string(index=False)\n",
    "        ipastr = ipastr + ipatoken + \" \"\n",
    "        idx += len(ma.group())\n",
    "    return ipastr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Major program starts here\n",
    "\n",
    "(cdict, EHowTree) = read_databases()\n",
    "(sampapattern, sampa) = build_sampa_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese conversion complete\n"
     ]
    }
   ],
   "source": [
    "filename = \"testv4/01.TextGrid\"\n",
    "file_translation(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EU/phone Word\n"
     ]
    }
   ],
   "source": [
    "tg = read_txtgrid(filename+'.utf8')\n",
    "\n",
    "otiernames =  tg.get_tier_names()\n",
    "tiernames = [x.lower() for x in otiernames]\n",
    "\n",
    "idx = [i for i, x in enumerate(\"phone\" in x for x in tiernames) if x]\n",
    "phonetiername = otiernames[idx[0]]\n",
    "\n",
    "idx = [i for i, x in enumerate(\"word\" in x for x in tiernames) if x]\n",
    "wordtiername = otiernames[idx[0]]\n",
    "\n",
    "print(phonetiername, wordtiername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntervalTier(start_time=0.0, end_time=3.26, name=\"Word\", objects=[Interval(0.0, 0.15000000000000036, \"這\"), Interval(0.15000000000000036, 0.3800000000000008, \"是\"), Interval(0.3800000000000008, 0.5199999999999996, \"sp\"), Interval(0.5199999999999996, 0.7000000000000011, \"它\"), Interval(0.7000000000000011, 1.17, \"裏面\"), Interval(1.17, 1.3399999999999999, \"sp\"), Interval(1.3399999999999999, 1.4800000000000004, \"不\"), Interval(1.4800000000000004, 1.7400000000000002, \"大\"), Interval(1.7400000000000002, 1.92, \"最\"), Interval(1.92, 2.0199999999999996, \"不\"), Interval(2.0199999999999996, 2.24, \"大\"), Interval(2.24, 2.2699999999999996, \"sp\"), Interval(2.2699999999999996, 2.5299999999999994, \"一樣\"), Interval(2.5299999999999994, 2.620000000000001, \"的\"), Interval(2.620000000000001, 3.08, \"地方\"), Interval(3.08, 3.26, \"sp\")])\n",
      "IntervalTier(start_time=0.0, end_time=3.26, name=\"Word\", objects=[Interval(0.0, 0.15000000000000036, \"這\"), Interval(0.15000000000000036, 0.3800000000000008, \"是\"), Interval(0.3800000000000008, 0.5199999999999996, \"sp\"), Interval(0.5199999999999996, 0.7000000000000011, \"它\"), Interval(0.7000000000000011, 1.17, \"裏面\"), Interval(1.17, 1.3399999999999999, \"sp\"), Interval(1.3399999999999999, 1.4800000000000004, \"不\"), Interval(1.4800000000000004, 1.7400000000000002, \"大\"), Interval(1.7400000000000002, 1.92, \"最\"), Interval(1.92, 2.0199999999999996, \"不\"), Interval(2.0199999999999996, 2.24, \"大\"), Interval(2.24, 2.2699999999999996, \"sp\"), Interval(2.2699999999999996, 2.5299999999999994, \"一樣\"), Interval(2.5299999999999994, 2.620000000000001, \"的\"), Interval(2.620000000000001, 3.08, \"地方\"), Interval(3.08, 3.26, \"sp\")])\n",
      "IntervalTier(start_time=0.0, end_time=3.26, name=\"EU/phone\", objects=[Interval(0.0, 0.07000000000000028, \"Z\"), Interval(0.07000000000000028, 0.15000000000000036, \"&\"), Interval(0.15000000000000036, 0.2400000000000002, \"S\"), Interval(0.2400000000000002, 0.3800000000000008, \"%\"), Interval(0.3800000000000008, 0.5199999999999996, \"sp\"), Interval(0.5199999999999996, 0.6300000000000008, \"t\"), Interval(0.6300000000000008, 0.7000000000000011, \"a\"), Interval(0.7000000000000011, 0.75, \"l\"), Interval(0.75, 0.8000000000000007, \"i\"), Interval(0.8000000000000007, 0.9299999999999997, \"m\"), Interval(0.9299999999999997, 1.0, \"y\"), Interval(1.0, 1.08, \"E\"), Interval(1.08, 1.17, \"n\"), Interval(1.17, 1.3399999999999999, \"sp\"), Interval(1.3399999999999999, 1.3900000000000006, \"b\"), Interval(1.3900000000000006, 1.4800000000000004, \"u\"), Interval(1.4800000000000004, 1.5299999999999994, \"d\"), Interval(1.5299999999999994, 1.7400000000000002, \"a\"), Interval(1.7400000000000002, 1.7699999999999996, \"z\"), Interval(1.7699999999999996, 1.8000000000000007, \"w\"), Interval(1.8000000000000007, 1.8499999999999996, \"e\"), Interval(1.8499999999999996, 1.92, \"y\"), Interval(1.92, 1.9600000000000009, \"b\"), Interval(1.9600000000000009, 2.0199999999999996, \"u\"), Interval(2.0199999999999996, 2.08, \"d\"), Interval(2.08, 2.210000000000001, \"@\"), Interval(2.210000000000001, 2.24, \"y\"), Interval(2.24, 2.2699999999999996, \"sp\"), Interval(2.2699999999999996, 2.34, \"i\"), Interval(2.34, 2.4000000000000004, \"y\"), Interval(2.4000000000000004, 2.4700000000000006, \"a\"), Interval(2.4700000000000006, 2.5299999999999994, \"N\"), Interval(2.5299999999999994, 2.5600000000000005, \"d\"), Interval(2.5600000000000005, 2.620000000000001, \"&\"), Interval(2.620000000000001, 2.6799999999999997, \"d\"), Interval(2.6799999999999997, 2.8499999999999996, \"i\"), Interval(2.8499999999999996, 2.9000000000000004, \"f\"), Interval(2.9000000000000004, 3.0, \"a\"), Interval(3.0, 3.08, \"N\"), Interval(3.08, 3.26, \"sp\")])\n"
     ]
    }
   ],
   "source": [
    "ophonetier = tg.get_tier_by_name(phonetiername)\n",
    "owordtier =  tg.get_tier_by_name(wordtiername)\n",
    "\n",
    "phonetier = ophonetier.get_copy_with_gaps_filled()\n",
    "wordtier  = owordtier.get_copy_with_gaps_filled()\n",
    "\n",
    "nwordtier  = owordtier.get_copy_with_gaps_filled()\n",
    "nphonetier = ophonetier.get_copy_with_gaps_filled()\n",
    "\n",
    "remove_noise(nwordtier)\n",
    "remove_noise(nphonetier)\n",
    "\n",
    "print(nwordtier)\n",
    "\n",
    "filling_gaps(nwordtier)\n",
    "filling_gaps(nphonetier)\n",
    "\n",
    "align_tiers(nwordtier, nphonetier)\n",
    "\n",
    "pst = phonetier.start_time\n",
    "pet = phonetier.end_time\n",
    "\n",
    "wst = wordtier.start_time\n",
    "wet = wordtier.end_time\n",
    "\n",
    "st = pst\n",
    "et = pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntervalTier(start_time=0.0, end_time=3.26, name=\"Word\", objects=[Interval(0.0, 0.15000000000000036, \"這\"), Interval(0.15000000000000036, 0.3800000000000008, \"是\"), Interval(0.3800000000000008, 0.5199999999999996, \"sp\"), Interval(0.5199999999999996, 0.7000000000000011, \"它\"), Interval(0.7000000000000011, 1.17, \"裏面\"), Interval(1.17, 1.3399999999999999, \"sp\"), Interval(1.3399999999999999, 1.4800000000000004, \"不\"), Interval(1.4800000000000004, 1.7400000000000002, \"大\"), Interval(1.7400000000000002, 1.92, \"最\"), Interval(1.92, 2.0199999999999996, \"不\"), Interval(2.0199999999999996, 2.24, \"大\"), Interval(2.24, 2.2699999999999996, \"sp\"), Interval(2.2699999999999996, 2.5299999999999994, \"一樣\"), Interval(2.5299999999999994, 2.620000000000001, \"的\"), Interval(2.620000000000001, 3.08, \"地方\"), Interval(3.08, 3.26, \"sp\")])\n",
      "---\n",
      "IntervalTier(start_time=0.0, end_time=3.26, name=\"EU/phone\", objects=[Interval(0.0, 0.07000000000000028, \"Z\"), Interval(0.07000000000000028, 0.15000000000000036, \"&\"), Interval(0.15000000000000036, 0.2400000000000002, \"S\"), Interval(0.2400000000000002, 0.3800000000000008, \"%\"), Interval(0.3800000000000008, 0.5199999999999996, \"sp\"), Interval(0.5199999999999996, 0.6300000000000008, \"t\"), Interval(0.6300000000000008, 0.7000000000000011, \"a\"), Interval(0.7000000000000011, 0.75, \"l\"), Interval(0.75, 0.8000000000000007, \"i\"), Interval(0.8000000000000007, 0.9299999999999997, \"m\"), Interval(0.9299999999999997, 1.0, \"y\"), Interval(1.0, 1.08, \"E\"), Interval(1.08, 1.17, \"n\"), Interval(1.17, 1.3399999999999999, \"sp\"), Interval(1.3399999999999999, 1.3900000000000006, \"b\"), Interval(1.3900000000000006, 1.4800000000000004, \"u\"), Interval(1.4800000000000004, 1.5299999999999994, \"d\"), Interval(1.5299999999999994, 1.7400000000000002, \"a\"), Interval(1.7400000000000002, 1.7699999999999996, \"z\"), Interval(1.7699999999999996, 1.8000000000000007, \"w\"), Interval(1.8000000000000007, 1.8499999999999996, \"e\"), Interval(1.8499999999999996, 1.92, \"y\"), Interval(1.92, 1.9600000000000009, \"b\"), Interval(1.9600000000000009, 2.0199999999999996, \"u\"), Interval(2.0199999999999996, 2.08, \"d\"), Interval(2.08, 2.210000000000001, \"@\"), Interval(2.210000000000001, 2.24, \"y\"), Interval(2.24, 2.2699999999999996, \"sp\"), Interval(2.2699999999999996, 2.34, \"i\"), Interval(2.34, 2.4000000000000004, \"y\"), Interval(2.4000000000000004, 2.4700000000000006, \"a\"), Interval(2.4700000000000006, 2.5299999999999994, \"N\"), Interval(2.5299999999999994, 2.5600000000000005, \"d\"), Interval(2.5600000000000005, 2.620000000000001, \"&\"), Interval(2.620000000000001, 2.6799999999999997, \"d\"), Interval(2.6799999999999997, 2.8499999999999996, \"i\"), Interval(2.8499999999999996, 2.9000000000000004, \"f\"), Interval(2.9000000000000004, 3.0, \"a\"), Interval(3.0, 3.08, \"N\"), Interval(3.08, 3.26, \"sp\")])\n",
      "0.0 3.26\n"
     ]
    }
   ],
   "source": [
    "print(wordtier)\n",
    "print(\"---\")\n",
    "print(phonetier)\n",
    "\n",
    "#ann = wordtier.get_annotation_by_start_time(0)\n",
    "#print(ann)\n",
    "#ann = phonetier.get_annotation_by_start_time(0)\n",
    "#print(ann)\n",
    "\n",
    "print(st, et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Interval(0.0, 0.15000000000000036, \"這\"), Interval(0.15000000000000036, 0.3800000000000008, \"是\"), Interval(0.5199999999999996, 0.7000000000000011, \"它\"), Interval(0.7000000000000011, 1.17, \"裏面\"), Interval(1.3399999999999999, 1.4800000000000004, \"不\"), Interval(1.4800000000000004, 1.7400000000000002, \"大\"), Interval(1.7400000000000002, 1.92, \"最\"), Interval(1.92, 2.0199999999999996, \"不\"), Interval(2.0199999999999996, 2.24, \"大\"), Interval(2.2699999999999996, 2.5299999999999994, \"一樣\"), Interval(2.5299999999999994, 2.620000000000001, \"的\"), Interval(2.620000000000001, 3.08, \"地方\")]\n"
     ]
    }
   ],
   "source": [
    "# Extract all tha annotations from word layer\n",
    "\n",
    "timeptr = st\n",
    "ann = wordtier.get_annotation_by_start_time(timeptr)\n",
    "annlist = []\n",
    "while (timeptr < et):\n",
    "    if ann is not None:\n",
    "        if (ann.text != 'sp'): \n",
    "            #print(ann.text)\n",
    "            annlist.append(ann)\n",
    "        timeptr = timeptr + ann.duration()\n",
    "        #print(timeptr)\n",
    "        ann = wordtier.get_annotation_by_start_time(timeptr)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(annlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "這是它裏面不大最不大一樣的地方\n"
     ]
    }
   ],
   "source": [
    "# 處理第一層中文層\n",
    "\n",
    "ctext = \"\"\n",
    "\n",
    "for a in annlist:\n",
    "    if (contain_chinese(a.text)):\n",
    "        ctext = ctext + a.text\n",
    "        # print(a.text)\n",
    "    else:\n",
    "        if (ctext[-1] == \" \"):\n",
    "            ctext = ctext + a.text + \" \"\n",
    "        else:\n",
    "            ctext = ctext + \" \" + a.text + \" \"\n",
    "            \n",
    "#print(ctext)\n",
    "ctier = tgt.core.PointTier(st, et, u\"IU/逐字稿\")\n",
    "#add_text_tier(newtg, u\"IU/逐字稿\")\n",
    "cann = tgt.core.Point((et-st)/2.0, ctext)\n",
    "ctier.add_annotation(cann)\n",
    "print(ctext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is not the same as inside it is not the best place\n"
     ]
    }
   ],
   "source": [
    "# 處理第二層英文層\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "tr = translator.translate(ctext, dest = 'en')\n",
    "print (tr.text)\n",
    "\n",
    "etier = tgt.core.PointTier(st, et, \"English\")\n",
    "#add_text_tier(newtg, \"English\")\n",
    "eann = tgt.core.Point((et-st)/2.0,tr.text)\n",
    "etier.add_annotation(eann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntervalTier(start_time=0.0, end_time=3.26, name=\"IU/Word\", objects=[Annotation(0.0, 0.15000000000000036, \"Nep\"), Annotation(0.15000000000000036, 0.3800000000000008, \"SHI,SHI D\"), Annotation(0.3800000000000008, 0.5199999999999996, \"sp\"), Annotation(0.5199999999999996, 0.7000000000000011, \"Nh\"), Annotation(0.7000000000000011, 1.17, \"Ncd\"), Annotation(1.17, 1.3399999999999999, \"sp\"), Annotation(1.3399999999999999, 1.4800000000000004, \"D\"), Annotation(1.4800000000000004, 1.7400000000000002, \"VH Dfa\"), Annotation(1.7400000000000002, 1.92, \"Dfa\"), Annotation(1.92, 2.0199999999999996, \"D\"), Annotation(2.0199999999999996, 2.24, \"VH Dfa\"), Annotation(2.24, 2.2699999999999996, \"sp\"), Annotation(2.2699999999999996, 2.5299999999999994, \"VH Ng\"), Annotation(2.5299999999999994, 2.620000000000001, \"Na T\"), Annotation(2.620000000000001, 3.08, \"A Na\"), Annotation(3.08, 3.26, \"sp\")])\n"
     ]
    }
   ],
   "source": [
    "# 處理第三層 加入詞意層\n",
    "#postier = add_interval_tier(newtg, \"IU/Word\", st, et)\n",
    "postier = tgt.core.IntervalTier(st, et, \"IU/Word\")\n",
    "timeptr = st\n",
    "ann = wordtier.get_annotation_by_start_time(timeptr)\n",
    "while (timeptr < et):\n",
    "    newann = tgt.core.Annotation(ann.start_time, ann.end_time, text=\" \")\n",
    "    if (contain_chinese(ann.text)): \n",
    "        newannstr = \"\"\n",
    "        #去 EHowNet 查詞性\n",
    "        posp=EHowTree.searchWord(ann.text)\n",
    "        #print(ann.text, posp)\n",
    "        if posp:\n",
    "            for w in posp:\n",
    "                #print(w.pos)\n",
    "                newannstr = newannstr + w.pos + \" \"\n",
    "            #print(node)\n",
    "            newann.text = newannstr.rstrip()\n",
    "    elif ann.text == 'sp':\n",
    "        newann.text = 'sp'\n",
    "    #print(newannstr)\n",
    "    #print(c, \"[\", newann.start_time, newann.end_time, \"]\", ss)\n",
    "    postier.add_annotation(newann)\n",
    "    timeptr = ann.end_time\n",
    "    ann = wordtier.get_annotation_by_start_time(timeptr)\n",
    "    if ann == None:\n",
    "        break\n",
    "    \n",
    "print(postier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z&  C  V \n",
      "S%  C  V \n",
      "ta  C  V \n",
      "limyEn  C  V  N  G  V  N \n",
      "bu  C  V \n",
      "da  C  V \n",
      "zwey  C  G  V  G \n",
      "bu  C  V \n",
      "d@y  C  V  G \n",
      "iyaN  V  G  V  N \n",
      "d&  C  V \n",
      "difaN  C  V  C  V  N \n",
      "IntervalTier(start_time=0.0, end_time=3.26, name=\"IU/Syllable\", objects=[])\n"
     ]
    }
   ],
   "source": [
    "#處理第五層 IU/Syllable (CGVN)層\n",
    "\n",
    "cgvntier = tgt.core.IntervalTier(st, et, \"IU/Syllable\")\n",
    "timeptr = st\n",
    "ann = wordtier.get_annotation_by_start_time(timeptr)\n",
    "\n",
    "while (timeptr < et):\n",
    "    #print(ann.text)\n",
    "    if contain_chinese(ann.text):\n",
    "        #詞含有中文，抓出音來\n",
    "        \n",
    "        pholist = phonetier.get_annotations_between_timepoints(ann.start_time, ann.end_time)\n",
    "        \n",
    "        pstr = \"\"\n",
    "        for p in pholist:\n",
    "            pstr = pstr + p.text\n",
    "        \n",
    "        print(pstr, end = \" \")\n",
    "        \n",
    "        cvgnstr = sampa_to_cgvn(pstr, sampa, sampapattern)\n",
    "        print(cvgnstr)\n",
    "    timeptr = ann.end_time\n",
    "    ann = wordtier.get_annotation_by_start_time(timeptr)\n",
    "    if not ann:\n",
    "        break\n",
    "\n",
    "print(cgvntier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#處理第五層 IU/Syllable (CGVN)層\n",
    "\n",
    "cgvntier = tgt.core.IntervalTier(st, et, \"IU/Syllable\")\n",
    "timeptr = st\n",
    "ann = wordtier.get_annotation_by_start_time(timeptr)\n",
    "\n",
    "while (timeptr < et):\n",
    "    #print(ann.text)\n",
    "    if contain_chinese(ann.text):\n",
    "        #詞含有中文，抓出音來\n",
    "        \n",
    "        pholist = phonetier.get_annotations_between_timepoints(ann.start_time, ann.end_time)\n",
    "        print(ann.text)\n",
    "        cgvnstr = ''\n",
    "        for p in pholist:\n",
    "            ptext = p.text\n",
    "            #print(ptext, end =\" \")\n",
    "            cgvn = sampadict.loc[sampadict['IPA'] == ptext, 'CGVN']\n",
    "            if not cgvn.empty:\n",
    "                cstring = cgvn.to_string(index=False).replace('\\n','')\n",
    "                #print(cstring)\n",
    "                cgvnstr = cgvnstr + cstring\n",
    "            #cgvnstr = cgvnstr.replace(\" \",\"\")\n",
    "            print(\">>> \", ptext, cgvnstr)\n",
    "    elif ann.text == \"sp\":\n",
    "        cgvnstr = \"sp\"\n",
    "    else:\n",
    "        cgvnstr = \" \"\n",
    "    cgvnstr = cgvnstr.replace(\" \",\"\")\n",
    "    newann = tgt.core.Annotation(ann.start_time, ann.end_time, cgvnstr)\n",
    "    cgvntier.add_annotation(newann)\n",
    "    timeptr = ann.end_time\n",
    "    ann = wordtier.get_annotation_by_start_time(timeptr)\n",
    "    if not ann:\n",
    "        break\n",
    "\n",
    "print(cgvntier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#第六層處理四聲層\n",
    "\n",
    "to1tier = tgt.core.IntervalTier(st, et,\"IU/Tone\")\n",
    "\n",
    "timeptr = st\n",
    "ann = wordtier.get_annotation_by_start_time(timeptr)\n",
    "while (timeptr < et):\n",
    "    #print(ann.text)\n",
    "    if (contain_chinese(ann.text)): \n",
    "        newannstr = ''\n",
    "        duration = ann.end_time - ann.start_time\n",
    "        intv = duration / len(ann.text)\n",
    "        cstart = ann.start_time\n",
    "        for c in ann.text:\n",
    "            newann = tgt.core.Annotation(cstart, cstart+intv, text='')\n",
    "            cstart = cstart + intv\n",
    "            pho = cdict.loc[cdict['character'] == c].phone\n",
    "            ss = pho.to_string(index=False).replace('\\n','')\n",
    "            newann.text = ss.lstrip()\n",
    "            print(c, ss)\n",
    "    elif ann.text == \"sp\":\n",
    "        newann = tgt.core.Annotation(ann.start_time, ann.end_time, text=\"sp\")\n",
    "    else:\n",
    "        newann = tgt.core.Annotation(ann.start_time, ann.end_time, text=\" \")\n",
    "        \n",
    "    to1tier.add_annotation(newann)\n",
    "    #print(ann.text, \"[\", newann.start_time, newann.end_time, \"]\", \"...\")\n",
    "    timeptr = ann.end_time\n",
    "    ann = wordtier.get_annotation_by_start_time(timeptr)\n",
    "    if not ann:\n",
    "        break\n",
    "    \n",
    "print(to1tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two tiers from original textgrid file\n",
    "import copy\n",
    "\n",
    "newtg = tgt.core.TextGrid()\n",
    "\n",
    "newtg.add_tier(ctier)\n",
    "newtg.add_tier(etier)\n",
    "newtg.add_tier(wordtier)\n",
    "newtg.add_tier(postier)\n",
    "newtg.add_tier(ipatier)\n",
    "newtg.add_tier(cgvntier)\n",
    "newtg.add_tier(to1tier)\n",
    "\n",
    "newipatier = copy.copy(ipatier)\n",
    "newipatier.name = 'EU/Phone'\n",
    "newtg.add_tier(newipatier)\n",
    "\n",
    "newcgvntier = copy.copy(cgvntier)\n",
    "newcgvntier.name = 'EU/Syllable'\n",
    "newtg.add_tier(newcgvntier)\n",
    "\n",
    "newtonetier = copy.copy(to1tier)\n",
    "newtonetier.name = 'EU/Tone'\n",
    "newtg.add_tier(newtonetier)\n",
    "\n",
    "\n",
    "typetier = tgt.core.IntervalTier(st, et, \"EU/Type\")\n",
    "newtg.add_tier(typetier)\n",
    "\n",
    "subjecttier = tgt.core.IntervalTier(st, et, \"Subject\")\n",
    "newtg.add_tier(subjecttier)\n",
    "\n",
    "print_tier(newtg)\n",
    "write_txtgrid(newtg, \"A0303.textgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加上原來兩層\n",
    "\n",
    "newtg.add_tier(phonetier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ctier)\n",
    "print(etier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def main():\n",
    "    print('Number of arguments:', len(sys.argv), 'arguments.')\n",
    "    if (len(sys.argv) != 2):\n",
    "        print(\"Usage: sys.argv[0]: textfile, texgridfile\")\n",
    "    print('Argument List:', str(sys.argv))\n",
    "    print(\"Hello World!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opencc import OpenCC\n",
    "\n",
    "def translate():\n",
    "    cc = OpenCC('s2t')\n",
    "    source = open('03052019_1_009_conversion完.TextGrid.orig', 'r', encoding = 'utf-16')\n",
    "    result = open('test_translated.txt', 'w', encoding = 'utf-8')\n",
    "    # source就放純文字檔，轉完就放進去result\n",
    "    count = 0\n",
    "    while True:\n",
    "        line = source.readline()\n",
    "        line = cc.convert(line)\n",
    "        if not line:  #readline會一直讀下去，這邊做的break\n",
    "            break\n",
    "        #print(line)\n",
    "        count = count +1\n",
    "        result.write(line) \n",
    "        #print('===已處理'+str(count)+'行===')\n",
    "    source.close()        \n",
    "    result.close()\n",
    "    \n",
    "translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"那\"\n",
    "for c in str:\n",
    "    p = dict.loc[dict['character'] == c].phone\n",
    "    ss = p.to_string(index=False).replace('\\n','')\n",
    "    #print(ss)\n",
    "    print(c,ss)\n",
    "    \n",
    "#print(dict.phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#處理第五層 IU/Syllable (CGVN)層\n",
    "remove_old_tier(newtg, \"IU/Syllable\")\n",
    "\n",
    "cgvntier = add_interval_tier(newtg, \"IU/Syllable\", st, et)\n",
    "timeptr = st\n",
    "ann = wordtier.get_annotation_by_start_time(timeptr)\n",
    "\n",
    "while (timeptr < et):\n",
    "    #print(ann.text)\n",
    "    if contain_chinese(ann.text):\n",
    "        intv = (ann.end_time - ann.start_time) / len(c)\n",
    "        cgvnstart =ann.start_time\n",
    "        cgvnend = cvgnstart + intv\n",
    "        for c in ann.text:\n",
    "            newann = tgt.core.Annotation(cgvnstart, cgvnend, text=\"\")\n",
    "            \n",
    "        pholist = phonetier.get_annotations_between_timepoints(ann.start_time, ann.end_time)\n",
    "        \n",
    "        cgvnstr = ''\n",
    "        for p in pholist:\n",
    "            cgvn = sampadict.loc[sampadict['SAMPA'] == p.text, 'CGVN']\n",
    "            if cgvn.empty:\n",
    "                cgvnstr = \" \"\n",
    "                break\n",
    "            else:\n",
    "                #print(p)\n",
    "                cgvnstr = cgvnstr + cgvn.to_string(index=False).replace('\\n','')\n",
    "            cgvnstr = cgvnstr.replace(\" \",\"\")\n",
    "    else:\n",
    "        cgvnstr = \" \"\n",
    "        \n",
    "    newann = tgt.core.Annotation(ann.start_time, ann.end_time, text=cgvnstr)\n",
    "    cvgntier.add_annotation(newann)\n",
    "    timeptr = ann.end_time\n",
    "    ann = wordtier.get_annotation_by_start_time(timeptr)\n",
    "\n",
    "print(cgvntier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#處理第五層 IU/Syllable (CGVN)層\n",
    "\n",
    "cgvntier = add_interval_tier(newtg, \"IU/Syllable\", st, et)\n",
    "timeptr = st\n",
    "ann = phonetier.get_annotation_by_start_time(timeptr)\n",
    "\n",
    "while (timeptr < et):\n",
    "    #print(ann.text)\n",
    "    cgvn = sampadict.loc[sampadict['SAMPA'] == ann.text, 'CGVN']\n",
    "    \n",
    "    if cgvn.empty:\n",
    "        cgvnstr = ann.text\n",
    "    else:\n",
    "        cgvnstr = cgvn.to_string(index=False).replace('\\n','')\n",
    "    \n",
    "    newann = tgt.core.Annotation(ann.start_time, ann.end_time, text=cgvnstr)\n",
    "    cgvntier.add_annotation(newann)\n",
    "    timeptr = ann.end_time\n",
    "    ann = phonetier.get_annotation_by_start_time(timeptr)\n",
    "    if not ann:\n",
    "        break\n",
    "\n",
    "print(cgvntier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#處理第四層 IU/Phone (IPA)層\n",
    "\n",
    "ipatier = tgt.core.IntervalTier(st, et, \"Word\")\n",
    "timeptr = st\n",
    "ann = phonetier.get_annotation_by_start_time(timeptr)\n",
    "\n",
    "while (timeptr < et):\n",
    "    if ann.text == \"sp\":\n",
    "        newann = tgt.core.Annotation(ann.start_time, ann.end_time, \"sp\")\n",
    "    else:\n",
    "        print(ann.text)\n",
    "        ipaa = sampadict.loc[sampadict['SAMPA'] == ann.text, 'IPA']\n",
    "    \n",
    "        if ipaa.empty:\n",
    "            ipaastr = ann.text\n",
    "        else:\n",
    "            ipaastr = ipaa.to_string(index=False).replace('\\n','')\n",
    "    \n",
    "        newann = tgt.core.Annotation(ann.start_time, ann.end_time, text=ipaastr)\n",
    "    \n",
    "    ipatier.add_annotation(newann)\n",
    "    timeptr = ann.end_time\n",
    "    ann = phonetier.get_annotation_by_start_time(timeptr)\n",
    "    if not ann:\n",
    "        break\n",
    "\n",
    "print(ipatier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
